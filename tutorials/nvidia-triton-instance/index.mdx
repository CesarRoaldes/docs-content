---
meta:
  title: Nvidia Triton Server
  description: Nvidia Triton Server
content:
  h1: Nvidia Triton Server
  paragraph: Nvidia Triton Server
tags: instance gpu nvidia triton
categories:
  - compute
  - instance
dates:
  validation: 2023-08-01
  posted: 2023-08-01
---

NVIDIA Triton Inference Server serves as an open-source software designed to streamline the deployment of trained AI models at a large scale in production.
Through its HTTP or gRPC endpoints, clients can conveniently send inference requests for any model managed by the server.

One of the remarkable features of NVIDIA Triton is its ability to handle a diverse range of models, subject to system disk and memory resources.
Additionally, it supports various deep-learning frameworks like TensorFlow, PyTorch, NVIDIA TensorRT, and more. This empowers developers and data scientists, freeing them from the constraints of using a particular model framework.

<Message type="requirement">
  - You have an account and are logged into the [Scaleway console](https://console.scaleway.com)
  - You have installed [Docker CE](/tutorials/install-docker-ubuntu-bionic/) on the Instance
</Message>

