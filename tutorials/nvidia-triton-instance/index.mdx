---
meta:
  title: Nvidia Triton Server
  description: Nvidia Triton Server
content:
  h1: Nvidia Triton Server
  paragraph: Nvidia Triton Server
tags: instance gpu nvidia triton
categories:
  - compute
  - instance
dates:
  validation: 2023-08-01
  posted: 2023-08-01
---

NVIDIA Triton Inference Server serves as an open-source software designed to streamline the deployment of trained AI models at a large scale in production.
Through its HTTP or gRPC endpoints, clients can conveniently send inference requests for any model managed by the server.

One of the remarkable features of NVIDIA Triton is its ability to handle a diverse range of models, subject to system disk and memory resources.
Additionally, it supports various deep-learning frameworks like TensorFlow, PyTorch, NVIDIA TensorRT, and more. This empowers developers and data scientists, freeing them from the constraints of using a particular model framework.

Triton is optimized for [GPU Instances](link) but it can also run [Instances](link) by using the CPU.

<Message type="requirement">
  - You have an account and are logged into the [Scaleway console](https://console.scaleway.com)
  - You have installed [Docker Engine](/tutorials/install-docker-ubuntu-jammy-jellyfish/) on the Instance
</Message>

1. Login to your Instance using SSH.
    ```
    ssh root@<YOUR_INSTANCE_IP>
    ```
2. Clone the Triton server repository unsing `git`:
    ```
    git clone -b r23.07 https://github.com/triton-inference-server/server.git
    ```
3. Enter the example directory to download an example model:
    ```
    cd server/docs/examples
    ```
4. Run the following script to download the example model:
    ```
    ./fetch_models.sh
    ```
5. Launch Triton from the NGC Triton container:
    ```
    docker run --gpus=1 --rm --net=host -v ${PWD}/model_repository:/models nvcr.io/nvidia/tritonserver:23.07-py3 tritonserver --model-repository=/models
    ```
    <Message type="note">
        If you want to run Triton on an Instance with no GPU access, run the following command:
        ```
        docker run --rm --net=host -v ${PWD}/model_repository:/models nvcr.io/nvidia/tritonserver:23.07-py3 tritonserver --model-repository=/models
        ```
    </Message>
6. Open another terminal window and send an Inference request 